"""
PDF VLMå¤„ç†å™¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ V5
ä¸“æ³¨è§£å†³æ€§èƒ½ç“¶é¢ˆï¼šæ¿€è¿›å¹¶å‘+æ™ºèƒ½ç¼“å­˜+å¿«é€Ÿå¤±è´¥
è·¯å¾„ï¼šagent/utils/pdf_vlm_processor_v5_optimized.py
"""

# type: ignore

import os
import sys
import time
import asyncio
import aiohttp
import json
import base64
import hashlib
import threading
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Any
from pdf2image import convert_from_path
from PIL import Image
import multiprocessing as mp
import concurrent.futures
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PerformanceCache:
    """é«˜æ€§èƒ½ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, max_size: int = 200):
        self.cache: Dict[str, str] = {}
        self.access_times: Dict[str, float] = {}
        self.max_size = max_size
        self.lock = threading.Lock()
        self.hit_count = 0
        self.miss_count = 0
    
    def get_image_base64(self, image_path: str) -> str:
        """è·å–å›¾ç‰‡Base64ç¼–ç ï¼ˆå¸¦LRUç¼“å­˜ï¼‰"""
        # è®¡ç®—æ–‡ä»¶å“ˆå¸Œä½œä¸ºç¼“å­˜é”®
        cache_key = hashlib.md5(image_path.encode()).hexdigest()
        
        with self.lock:
            if cache_key in self.cache:
                self.access_times[cache_key] = time.time()
                self.hit_count += 1
                return self.cache[cache_key]
            
            # ç¼“å­˜æœªå‘½ä¸­
            self.miss_count += 1
            
            try:
                with open(image_path, "rb") as f:
                    image_data = base64.b64encode(f.read()).decode('utf-8')
                    image_url = f"data:image/png;base64,{image_data}"
                
                # æ·»åŠ åˆ°ç¼“å­˜
                if len(self.cache) >= self.max_size:
                    self._evict_oldest()
                
                self.cache[cache_key] = image_url
                self.access_times[cache_key] = time.time()
                return image_url
                
            except Exception as e:
                logger.error(f"è¯»å–å›¾ç‰‡å¤±è´¥ {image_path}: {e}")
                raise
    
    def _evict_oldest(self):
        """åˆ é™¤æœ€ä¹…æœªä½¿ç”¨çš„ç¼“å­˜é¡¹"""
        if self.access_times:
            oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
            del self.cache[oldest_key]
            del self.access_times[oldest_key]
    
    def get_stats(self) -> Dict[str, int]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total = self.hit_count + self.miss_count
        hit_rate = self.hit_count / max(total, 1) * 100
        return {
            'hits': self.hit_count,
            'misses': self.miss_count, 
            'hit_rate': f"{hit_rate:.1f}%",
            'cache_size': len(self.cache)
        }

async def ultra_fast_vlm_request(session: aiohttp.ClientSession, 
                                page_num: int, 
                                image_path: str,
                                api_key: str,
                                model: str,
                                cache: PerformanceCache,
                                semaphore: asyncio.Semaphore) -> Tuple[int, str, bool, float]:
    """è¶…å¿«é€ŸVLMè¯·æ±‚"""
    async with semaphore:
        start_time = time.time()
        
        try:
            # ä»ç¼“å­˜è·å–å›¾ç‰‡
            image_data = cache.get_image_base64(image_path)
            
            payload = {
                "model": model,
                "messages": [
                    {"role": "system", "content": "ä½ å¿…é¡»å¿«é€Ÿç²¾å‡†æå–PDFå†…å®¹ã€‚"},
                    {"role": "user", "content": [
                        {"type": "text", "text": "è¯·é˜…è¯»PDFå›¾ç‰‡ï¼Œç”¨markdownæ ¼å¼è¿”å›æ‰€æœ‰ä¿¡æ¯ã€‚å›¾ç‰‡æ˜¯è‹±æ–‡åˆ™è¾“å‡ºè‹±æ–‡ã€‚"},
                        {"type": "image_url", "image_url": {"url": image_data}}
                    ]}
                ],
                "temperature": 0.5,
                "max_tokens": 8192,
                "stream": False
            }
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            # çŸ­è¶…æ—¶ï¼Œå¿«é€Ÿå¤±è´¥
            timeout = aiohttp.ClientTimeout(total=25, connect=5, sock_read=20)
            
            async with session.post(
                "https://ark.cn-beijing.volces.com/api/v3/chat/completions",
                json=payload,
                headers=headers,
                timeout=timeout
            ) as response:
                
                if response.status == 200:
                    result = await response.json()
                    content = result['choices'][0]['message']['content']
                    processing_time = time.time() - start_time
                    
                    logger.debug(f"é¡µé¢ {page_num} æˆåŠŸï¼Œè€—æ—¶ {processing_time:.2f}s")
                    return page_num, content, True, processing_time
                else:
                    error_text = await response.text()
                    raise Exception(f"APIé”™è¯¯ {response.status}: {error_text}")
                    
        except Exception as e:
            processing_time = time.time() - start_time
            logger.warning(f"é¡µé¢ {page_num} å¤±è´¥: {e}")
            return page_num, "", False, processing_time

async def process_batch_ultra_fast(image_paths: List[Tuple[int, str]], 
                                  api_key: str, 
                                  model: str,
                                  max_concurrent: int = 30) -> Dict[int, str]:
    """è¶…å¿«é€Ÿæ‰¹é‡å¤„ç†"""
    
    # åˆ›å»ºç¼“å­˜
    cache = PerformanceCache(max_size=len(image_paths) + 50)
    
    # æ§åˆ¶å¹¶å‘æ•°
    semaphore = asyncio.Semaphore(max_concurrent)
    
    # ä¼˜åŒ–è¿æ¥å™¨é…ç½®
    connector = aiohttp.TCPConnector(
        limit=max_concurrent + 10,
        limit_per_host=max_concurrent,
        ttl_dns_cache=300,
        use_dns_cache=True,
        keepalive_timeout=60,
        enable_cleanup_closed=True
    )
    
    timeout = aiohttp.ClientTimeout(total=30, connect=8, sock_read=22)
    
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [
            ultra_fast_vlm_request(session, page_num, image_path, api_key, model, cache, semaphore)
            for page_num, image_path in image_paths
        ]
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        page_contents = {}
        success_count = 0
        total_time = 0
        
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"ä»»åŠ¡å¼‚å¸¸: {result}")
                continue
                
            page_num, content, success, proc_time = result
            total_time += proc_time
            
            if success and content.strip():
                page_contents[page_num] = content
                success_count += 1
        
        # è¾“å‡ºç¼“å­˜ç»Ÿè®¡
        cache_stats = cache.get_stats()
        avg_time = total_time / len(results) if results else 0
        
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: {success_count}/{len(image_paths)} æˆåŠŸ")
        logger.info(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}s")
        logger.info(f"ç¼“å­˜ç»Ÿè®¡: {cache_stats}")
        
        return page_contents

def process_batch_worker(args: Tuple[List[Tuple[int, str]], str, str, int]) -> Dict[int, str]:
    """æ‰¹é‡å¤„ç†å·¥ä½œè¿›ç¨‹"""
    image_paths, api_key, model, max_concurrent = args
    
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        return loop.run_until_complete(
            process_batch_ultra_fast(image_paths, api_key, model, max_concurrent)
        )
    finally:
        loop.close()

def convert_pdf_to_images_fast(pdf_path: str, output_dir: str, dpi: int = 150) -> List[Tuple[int, str]]:
    """å¿«é€ŸPDFè½¬å›¾ç‰‡"""
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–é¡µæ•°
    try:
        import fitz
        doc = fitz.open(pdf_path)
        total_pages = doc.page_count
        doc.close()
    except:
        total_pages = 100  # é»˜è®¤å€¼ï¼Œå®é™…è¿è¡Œæ—¶ä¼šè°ƒæ•´
    
    logger.info(f"å¼€å§‹è½¬æ¢PDFï¼Œé¢„è®¡ {total_pages} é¡µ")
    
    # æ‰¹é‡è½¬æ¢
    batch_size = 10
    all_results = []
    file_prefix = Path(pdf_path).stem
    
    for start in range(1, total_pages + 1, batch_size):
        end = min(start + batch_size - 1, total_pages)
        
        try:
            images = convert_from_path(pdf_path, dpi=dpi, first_page=start, last_page=end)
            
            for i, image in enumerate(images):
                page_num = start + i
                image_path = os.path.join(output_dir, f"{file_prefix}_page_{page_num:04d}.png")
                
                # ä¼˜åŒ–å›¾ç‰‡ä¿å­˜
                if image.size[0] > 1800 or image.size[1] > 1800:
                    ratio = 1800 / max(image.size)
                    new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
                    image = image.resize(new_size, Image.Resampling.LANCZOS)
                
                image.save(image_path, 'PNG', optimize=True, compress_level=6)
                all_results.append((page_num, image_path))
                
        except Exception as e:
            logger.warning(f"è½¬æ¢é¡µé¢ {start}-{end} å¤±è´¥: {e}")
            break  # åˆ°è¾¾æ–‡ä»¶æœ«å°¾
    
    logger.info(f"PDFè½¬æ¢å®Œæˆï¼Œç”Ÿæˆ {len(all_results)} å¼ å›¾ç‰‡")
    return all_results

class PDFVLMProcessorV5:
    """PDF VLMå¤„ç†å™¨ V5 - æ€§èƒ½ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, 
                 batch_size: int = 20,           # æ›´å¤§æ‰¹æ¬¡
                 max_concurrent: int = 35,       # æ›´é«˜å¹¶å‘  
                 max_workers: int = 6,           # æ›´å¤šè¿›ç¨‹
                 dpi: int = 150,                 # é€‚ä¸­DPI
                 model: str = "doubao-seed-1-6-flash-250615"):
        
        self.batch_size = batch_size
        self.max_concurrent = max_concurrent
        self.max_workers = max_workers
        self.dpi = dpi
        self.model = model
        
        # æ ¹æ®ç³»ç»Ÿèµ„æºä¼˜åŒ–
        cpu_count = mp.cpu_count()
        self.max_workers = min(max_workers, cpu_count)
        
        logger.info(f"åˆå§‹åŒ–V5å¤„ç†å™¨ - æ‰¹æ¬¡: {batch_size}, å¹¶å‘: {max_concurrent}, è¿›ç¨‹: {self.max_workers}")
    
    def process_pdf_to_markdown(self, pdf_path: str, output_md_path: str, 
                               temp_dir: Optional[str] = None,
                               cleanup: bool = True) -> str:
        """å®Œæ•´çš„PDFå¤„ç†æµç¨‹"""
        
        start_time = time.time()
        
        if temp_dir is None:
            temp_dir = os.path.join(os.path.dirname(output_md_path), "temp_images_v5")
        
        try:
            # æ­¥éª¤1: å¿«é€ŸPDFè½¬å›¾ç‰‡
            logger.info("ğŸš€ æ­¥éª¤1: å¿«é€ŸPDFè½¬å›¾ç‰‡")
            image_paths = convert_pdf_to_images_fast(pdf_path, temp_dir, self.dpi)
            
            if not image_paths:
                raise ValueError("PDFè½¬æ¢å¤±è´¥")
            
            # æ­¥éª¤2: è¶…å¿«é€ŸVLMæ‰¹é‡å¤„ç†
            logger.info("âš¡ æ­¥éª¤2: è¶…å¿«é€ŸVLMå¤„ç†")
            page_texts = self._process_vlm_multiprocess(image_paths)
            
            if not page_texts:
                raise ValueError("VLMå¤„ç†å¤±è´¥")
            
            # æ­¥éª¤3: ç”ŸæˆMarkdown
            logger.info("ğŸ“ æ­¥éª¤3: ç”ŸæˆMarkdown")
            markdown_content = self._generate_markdown(page_texts, len(image_paths))
            
            # ä¿å­˜æ–‡ä»¶
            os.makedirs(os.path.dirname(output_md_path), exist_ok=True)
            with open(output_md_path, "w", encoding="utf-8") as f:
                f.write(markdown_content)
            
            # æ€§èƒ½ç»Ÿè®¡
            total_time = time.time() - start_time
            success_rate = len(page_texts) / len(image_paths) * 100
            throughput = len(page_texts) / total_time
            
            logger.info("=" * 60)
            logger.info("ğŸ‰ V5å¤„ç†å®Œæˆ - æ€§èƒ½ç»Ÿè®¡")
            logger.info("=" * 60)
            logger.info(f"ğŸ“Š æ€»é¡µæ•°: {len(image_paths)}")
            logger.info(f"âœ… æˆåŠŸé¡µæ•°: {len(page_texts)}")
            logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
            logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}s")
            logger.info(f"ğŸš„ å¤„ç†é€Ÿåº¦: {throughput:.2f} é¡µ/ç§’")
            logger.info(f"ğŸ“„ å¹³å‡æ¯é¡µ: {total_time/len(image_paths):.2f}s")
            logger.info(f"ğŸ’¾ æ–‡ä»¶å¤§å°: {len(markdown_content):,} å­—ç¬¦")
            logger.info(f"ğŸ“ è¾“å‡ºè·¯å¾„: {output_md_path}")
            
            return markdown_content
            
        except Exception as e:
            logger.error(f"å¤„ç†å¤±è´¥: {e}")
            raise
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if cleanup and temp_dir and os.path.exists(temp_dir):
                try:
                    import shutil
                    shutil.rmtree(temp_dir)
                    logger.info(f"å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
                except Exception as e:
                    logger.warning(f"æ¸…ç†å¤±è´¥: {e}")
    
    def _process_vlm_multiprocess(self, image_paths: List[Tuple[int, str]]) -> Dict[int, str]:
        """å¤šè¿›ç¨‹VLMå¤„ç†"""
        
        api_key = os.environ.get("DOUBAO_API_KEY")
        if not api_key:
            raise ValueError("è¯·è®¾ç½® DOUBAO_API_KEY ç¯å¢ƒå˜é‡")
        
        # åˆ†æ‰¹å¤„ç†
        batches = []
        for i in range(0, len(image_paths), self.batch_size):
            batch = image_paths[i:i + self.batch_size]
            batches.append((batch, api_key, self.model, self.max_concurrent))
        
        logger.info(f"åˆ›å»º {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œä½¿ç”¨ {self.max_workers} ä¸ªè¿›ç¨‹")
        
        all_results = {}
        
        # ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_batch = {executor.submit(process_batch_worker, batch): i 
                              for i, batch in enumerate(batches)}
            
            for future in concurrent.futures.as_completed(future_to_batch):
                batch_idx = future_to_batch[future]
                try:
                    batch_results = future.result()
                    all_results.update(batch_results)
                    logger.info(f"æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)} å®Œæˆï¼Œ"
                               f"æˆåŠŸ {len(batch_results)} é¡µ")
                except Exception as e:
                    logger.error(f"æ‰¹æ¬¡ {batch_idx + 1} å¤±è´¥: {e}")
        
        return all_results
    
    def _generate_markdown(self, page_texts: Dict[int, str], total_pages: int) -> str:
        """ç”ŸæˆMarkdownå†…å®¹"""
        parts = []
        
        # æ·»åŠ å¤´éƒ¨ä¿¡æ¯
        parts.append("# PDFæ–‡æ¡£å†…å®¹\n\n")
        parts.append(f"*å¤„ç†æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}*\n\n")
        parts.append(f"*æˆåŠŸå¤„ç†: {len(page_texts)}/{total_pages} é¡µ*\n\n")
        parts.append("---\n\n")
        
        # æŒ‰é¡µç é¡ºåºæ·»åŠ å†…å®¹
        for page_num in sorted(page_texts.keys()):
            content = page_texts[page_num].strip()
            if content:
                parts.append(f"## ç¬¬ {page_num} é¡µ\n\n{content}\n\n")
        
        # æ·»åŠ å¤„ç†æŠ¥å‘Š
        missing_pages = set(range(1, total_pages + 1)) - set(page_texts.keys())
        if missing_pages:
            parts.append("---\n\n## å¤„ç†æŠ¥å‘Š\n\n")
            parts.append(f"**æœªæˆåŠŸå¤„ç†çš„é¡µé¢**: {sorted(missing_pages)}\n\n")
        
        return "".join(parts)

# ä¾¿æ·å‡½æ•°
def convert_pdf_to_markdown_v5_optimized(pdf_path: str, 
                                        output_md_path: str,
                                        batch_size: int = 20,
                                        max_concurrent: int = 35,
                                        max_workers: int = 6,
                                        dpi: int = 150,
                                        model: str = "doubao-seed-1-6-flash-250615",
                                        cleanup: bool = True) -> str:
    """
    V5ä¼˜åŒ–ç‰ˆPDFè½¬Markdown
    
    ä¸»è¦ä¼˜åŒ–ç‚¹ï¼š
    1. æ¿€è¿›å¹¶å‘é…ç½® (35å¹¶å‘)
    2. å¤§æ‰¹æ¬¡å¤„ç† (20å¼ /æ‰¹)
    3. æ™ºèƒ½å›¾ç‰‡ç¼“å­˜
    4. å¿«é€Ÿå¤±è´¥æœºåˆ¶
    5. å›¾ç‰‡å‹ç¼©ä¼˜åŒ–
    """
    
    processor = PDFVLMProcessorV5(
        batch_size=batch_size,
        max_concurrent=max_concurrent,
        max_workers=max_workers,
        dpi=dpi,
        model=model
    )
    
    return processor.process_pdf_to_markdown(
        pdf_path=pdf_path,
        output_md_path=output_md_path,
        cleanup=cleanup
    )

if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    if not os.environ.get("DOUBAO_API_KEY"):
        print("é”™è¯¯ï¼šè¯·è®¾ç½® DOUBAO_API_KEY ç¯å¢ƒå˜é‡")
        sys.exit(1)
    
    # æµ‹è¯•æ–‡ä»¶
    pdf_file = r"files/çˆ±å°”çœ¼ç§‘ï¼š2024å¹´å¹´åº¦æŠ¥å‘Š.pdf"  # ç›¸å¯¹è·¯å¾„
    output_file = r"files/çˆ±å°”çœ¼ç§‘_2024å¹´æŠ¥_v5optimized.md"
    
    try:
        logger.info("ğŸš€ å¼€å§‹V5ä¼˜åŒ–ç‰ˆå¤„ç†...")
        
        result = convert_pdf_to_markdown_v5_optimized(
            pdf_path=pdf_file,
            output_md_path=output_file,
            batch_size=20,         # å¤§æ‰¹æ¬¡
            max_concurrent=35,     # é«˜å¹¶å‘
            max_workers=6,         # å¤šè¿›ç¨‹
            dpi=150,              # é€‚ä¸­DPI
            cleanup=True
        )
        
        logger.info("ğŸ‰ V5ä¼˜åŒ–ç‰ˆå¤„ç†å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {e}")
        sys.exit(1) 