"""
æ–‡ä»¶åŠŸèƒ½ï¼šè±†åŒ…VLMå›¾ç‰‡è½¬æ–‡å­—å·¥å…· - å¿«é€Ÿæ‰¹é‡å¤„ç†ç‰ˆæœ¬
æ–‡ä»¶è·¯å¾„ï¼šbackend/utils/agent_tool_vlm_img2txt_doubao.py

ä¸»è¦åŠŸèƒ½ï¼š
1. æ‰¹é‡å¤„ç†å›¾ç‰‡æ–‡ä»¶å¤¹ï¼Œå°†å›¾ç‰‡è½¬æ¢ä¸ºMarkdownæ–‡æœ¬
2. æ”¯æŒå¼‚æ­¥å¹¶å‘å¤„ç†ï¼Œå¤§å¹…æå‡å¤„ç†é€Ÿåº¦
3. è‡ªåŠ¨è¿›åº¦ç›‘æ§å’Œé”™è¯¯é‡è¯•æœºåˆ¶
4. æ”¯æŒJupyterå’Œå¸¸è§„Pythonç¯å¢ƒ
5. ç›®æ ‡ï¼š400å¼ å›¾ç‰‡/åˆ†é’Ÿçš„å¤„ç†é€Ÿåº¦

ä½¿ç”¨æ–¹æ³•ï¼š
- å¸¸è§„Pythonç¯å¢ƒï¼šç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶
- Jupyterç¯å¢ƒï¼šè°ƒç”¨ quick_start_jupyter() å‡½æ•°

æ³¨æ„ï¼šã€æ¨¡å—åŒ–è®¾è®¡ã€‘æ­¤æ¨¡å—è¢«è®¾è®¡ä¸ºå¯å®‰å…¨å¯¼å…¥ï¼Œä¸ä¼šåœ¨å¯¼å…¥æ—¶æ‰§è¡Œå¼‚æ­¥ä»£ç 
"""

import os
import base64
import mimetypes
import asyncio
import aiohttp
import aiofiles
import time
import logging
from concurrent.futures import ThreadPoolExecutor
from textwrap import dedent
from pathlib import Path
from typing import List, Tuple, Optional
from dataclasses import dataclass
from volcenginesdkarkruntime import Ark
from dotenv import load_dotenv

load_dotenv()

# ============= é…ç½®å‚æ•° =============
# ã€é…ç½®å¤–ç½®ã€‘å°†æ‰€æœ‰ç¡¬ç¼–ç å‚æ•°æå–åˆ°æ­¤å¤„ï¼Œæ–¹ä¾¿ç»´æŠ¤å’Œè°ƒæ•´

# APIé…ç½®
DOUBAO_API_CONFIG = {
    "base_url": "https://ark.cn-beijing.volces.com/api/v3",
    "model_name": "doubao-seed-1-6-flash-250715",
    "temperature": 1,
    "top_p": 0.7,
    "max_tokens": 16384,
    "thinking_disabled": True
}

# æ€§èƒ½é…ç½®
PERFORMANCE_CONFIG = {
    "max_concurrent_requests": 30,       # é»˜è®¤æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    "max_retries": 3,                   # æœ€å¤§é‡è¯•æ¬¡æ•°
    "batch_size": 100,                  # é»˜è®¤æ‰¹æ¬¡å¤§å°
    "max_workers_sync": 20,             # åŒæ­¥ç‰ˆæœ¬çš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
    "connector_limit": 50,              # HTTPè¿æ¥æ± æ€»å¤§å°
    "connector_limit_per_host": 25,     # æ¯ä¸ªä¸»æœºçš„è¿æ¥æ•°
    "keepalive_timeout": 30,            # è¿æ¥ä¿æ´»è¶…æ—¶æ—¶é—´
    "request_timeout": 60,              # è¯·æ±‚æ€»è¶…æ—¶æ—¶é—´
    "connect_timeout": 10,              # è¿æ¥è¶…æ—¶æ—¶é—´
    "target_speed": 400                 # ç›®æ ‡å¤„ç†é€Ÿåº¦ï¼ˆå¼ /åˆ†é’Ÿï¼‰
}

# æ–‡ä»¶é…ç½®
FILE_CONFIG = {
    "default_image_folder": "files/aier",           # é»˜è®¤å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
    "log_file_path": "image_processing.log",       # æ—¥å¿—æ–‡ä»¶è·¯å¾„
    "report_file_path": "processing_report.md",    # å¤„ç†æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    "supported_extensions": {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff', '.webp'}  # æ”¯æŒçš„å›¾ç‰‡æ‰©å±•å
}

# æ—¥å¿—é…ç½®
LOG_CONFIG = {
    "level": logging.INFO,
    "format": '%(asctime)s - %(levelname)s - %(message)s',
    "encoding": 'utf-8'
}

# æç¤ºè¯é…ç½®
DOUBAO_VLM_PROMPT = dedent("""
    è¯·é˜…è¯»æˆ‘ä¸Šä¼ çš„pdfæ–‡ä»¶ï¼Œä½¿ç”¨markdownæ ¼å¼è¿”å›æ‰€æœ‰çš„ä¿¡æ¯ã€‚
    å¦‚æœæœ‰å›¾ç‰‡ï¼Œéœ€è¦ä½ ç”¨ä¸€ä¸ªmarkdownæ ‡é¢˜+æ–‡å­—æè¿°ï¼Œæ ‡é¢˜ä¸ºå›¾ç‰‡çš„æ ‡é¢˜ï¼Œæ–‡å­—æè¿°éœ€è¦è¯¦ç»†å…¨é¢åœ°ä»‹ç»è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚

    # æ³¨æ„ï¼š
    - ä½ çš„è¾“å‡ºå¿…é¡»ä¸åŸæ–‡çš„è¯­ç§ä¸€è‡´ã€‚å¦‚æœæˆ‘æä¾›çš„å›¾ç‰‡æ˜¯è‹±æ–‡ï¼Œä½ çš„è¾“å‡ºå¿…é¡»æ˜¯è‹±æ–‡ï¼›å¦‚æœæˆ‘æä¾›çš„å›¾ç‰‡æ˜¯ä¸­æ–‡ï¼Œä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ä¸­æ–‡ã€‚å³ï¼šä½ çš„è¾“å‡ºè¯­ç§è¦ä¸æˆ‘çš„å›¾ç‰‡è¯­ç§ä¸€è‡´ï¼Œä¸éœ€è¦åšç¿»è¯‘ã€‚

    å¼€å§‹è¾“å‡ºï¼š
""").strip()

# ============= ç¯å¢ƒæ£€æµ‹å’Œæ—¥å¿—åˆå§‹åŒ– =============

# ã€æ¶æ„è®¾è®¡ã€‘è¿è¡Œç¯å¢ƒæ£€æµ‹ï¼Œé¿å…ä¸Flaskç­‰æ¡†æ¶å†²çª
import sys
_FLASK_CONTEXT = 'flask' in sys.modules or 'werkzeug' in sys.modules

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=LOG_CONFIG["level"],
    format=LOG_CONFIG["format"],
    handlers=[
        logging.FileHandler(FILE_CONFIG["log_file_path"], encoding=LOG_CONFIG["encoding"]),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class ProcessingResult:
    image_path: str
    success: bool
    content: str = ""
    error: str = ""
    processing_time: float = 0.0

class FastImageProcessor:
    def __init__(self, max_concurrent_requests: int = None, max_retries: int = None):
        """
        åˆå§‹åŒ–å¿«é€Ÿå›¾ç‰‡å¤„ç†å™¨
        
        Args:
            max_concurrent_requests: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
        """
        self.max_concurrent_requests = max_concurrent_requests or PERFORMANCE_CONFIG["max_concurrent_requests"]
        self.max_retries = max_retries or PERFORMANCE_CONFIG["max_retries"]
        self.semaphore = asyncio.Semaphore(self.max_concurrent_requests)
        self.session: Optional[aiohttp.ClientSession] = None
        self.processed_count = 0
        self.total_count = 0
        self.start_time = 0
        
    async def __aenter__(self):
        # åˆ›å»ºæŒä¹…çš„HTTPä¼šè¯ï¼Œé…ç½®è¿æ¥æ± 
        connector = aiohttp.TCPConnector(
            limit=PERFORMANCE_CONFIG["connector_limit"],  # æ€»è¿æ¥æ± å¤§å°
            limit_per_host=PERFORMANCE_CONFIG["connector_limit_per_host"],  # æ¯ä¸ªä¸»æœºçš„è¿æ¥æ•°
            keepalive_timeout=PERFORMANCE_CONFIG["keepalive_timeout"],
            enable_cleanup_closed=True
        )
        timeout = aiohttp.ClientTimeout(total=PERFORMANCE_CONFIG["request_timeout"], connect=PERFORMANCE_CONFIG["connect_timeout"])
        self.session = aiohttp.ClientSession(
            connector=connector, 
            timeout=timeout,
            headers={'Content-Type': 'application/json'}
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ã€æ€§èƒ½è®¾è®¡ã€‘ç¡®ä¿HTTPä¼šè¯æ­£ç¡®å…³é—­ï¼Œé¿å…socketé”™è¯¯"""
        if self.session:
            try:
                await self.session.close()
                # ç­‰å¾…è¿æ¥å™¨å®Œå…¨å…³é—­
                if hasattr(self.session.connector, '_closed') and not self.session.connector.closed:
                    await asyncio.sleep(0.1)  # ç»™äºˆæ—¶é—´å®Œæˆæ¸…ç†
            except Exception as e:
                logger.warning(f"å…³é—­HTTPä¼šè¯æ—¶å‡ºç°è­¦å‘Š: {e}")
            finally:
                self.session = None

    def get_image_base64(self, image_path: str) -> str:
        """åŒæ­¥è·å–å›¾ç‰‡çš„base64ç¼–ç """
        try:
            with open(image_path, "rb") as image_file:
                image_data = image_file.read()
            
            mime_type, _ = mimetypes.guess_type(image_path)
            if mime_type is None:
                mime_type = "application/octet-stream"
            
            image_base64 = base64.b64encode(image_data).decode('utf-8')
            return f"data:{mime_type};base64,{image_base64}"
        except Exception as e:
            logger.error(f"è¯»å–å›¾ç‰‡å¤±è´¥ {image_path}: {e}")
            raise

    async def process_single_image_async(self, image_path: str, output_path: str) -> ProcessingResult:
        """å¼‚æ­¥å¤„ç†å•ä¸ªå›¾ç‰‡"""
        start_time = time.time()
        
        async with self.semaphore:  # é™åˆ¶å¹¶å‘æ•°
            try:
                # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒI/Oå¯†é›†å‹æ“ä½œ
                loop = asyncio.get_event_loop()
                with ThreadPoolExecutor(max_workers=PERFORMANCE_CONFIG["connector_limit_per_host"]) as executor:
                    image_base64 = await loop.run_in_executor(
                        executor, self.get_image_base64, image_path
                    )
                
                # å¼‚æ­¥è°ƒç”¨API
                content = await self.call_doubao_api_async(image_base64, image_path)
                
                # å¼‚æ­¥å†™å…¥æ–‡ä»¶
                async with aiofiles.open(output_path, "w", encoding="utf-8") as f:
                    await f.write(content)
                
                processing_time = time.time() - start_time
                self.processed_count += 1
                
                # è®¡ç®—è¿›åº¦å’Œé¢„ä¼°æ—¶é—´
                progress = (self.processed_count / self.total_count) * 100
                elapsed = time.time() - self.start_time
                avg_time_per_image = elapsed / self.processed_count
                remaining_images = self.total_count - self.processed_count
                eta = remaining_images * avg_time_per_image
                
                logger.info(
                    f"âœ… å®Œæˆ [{self.processed_count}/{self.total_count}] "
                    f"({progress:.1f}%) {Path(image_path).name} "
                    f"è€—æ—¶: {processing_time:.2f}s, "
                    f"é¢„è®¡å‰©ä½™: {eta:.1f}s, "
                    f"å¹³å‡é€Ÿåº¦: {avg_time_per_image:.2f}s/å›¾"
                )
                
                return ProcessingResult(
                    image_path=image_path,
                    success=True,
                    content=content,
                    processing_time=processing_time
                )
                
            except Exception as e:
                processing_time = time.time() - start_time
                error_msg = f"å¤„ç†å›¾ç‰‡å¤±è´¥: {e}"
                logger.error(f"âŒ {Path(image_path).name}: {error_msg}")
                
                return ProcessingResult(
                    image_path=image_path,
                    success=False,
                    error=error_msg,
                    processing_time=processing_time
                )

    async def call_doubao_api_async(self, image_base64: str, image_path: str) -> str:
        """å¼‚æ­¥è°ƒç”¨è±†åŒ…API"""
        conversations = [
            {"role": "system", "content": "ä½ å¿…é¡»ç²¾å‡†æå–PDFå›¾ç‰‡çš„å†…å®¹ã€‚"}, 
            {"role": "user", "content": [
                {"type": "text", "text": DOUBAO_VLM_PROMPT},
                {"type": "image_url", "image_url": {"url": image_base64}}
            ]}
        ]
        
        for attempt in range(self.max_retries):
            try:
                # ä½¿ç”¨åŒæ­¥çš„volcengine SDKï¼Œä½†åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œ
                loop = asyncio.get_event_loop()
                with ThreadPoolExecutor(max_workers=min(PERFORMANCE_CONFIG["connector_limit_per_host"], 4)) as executor:
                    content = await loop.run_in_executor(
                        executor, self._sync_call_doubao, conversations
                    )
                return content
                
            except Exception as e:
                if attempt < self.max_retries - 1:
                    wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                    logger.warning(f"âš ï¸  {Path(image_path).name} APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {e}, {wait_time}såé‡è¯•")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(f"âŒ {Path(image_path).name} APIè°ƒç”¨æœ€ç»ˆå¤±è´¥: {e}")
                    raise

    def _sync_call_doubao(self, conversations) -> str:
        """åŒæ­¥è°ƒç”¨è±†åŒ…API"""
        client = Ark(
            base_url=DOUBAO_API_CONFIG["base_url"],
            api_key=os.environ.get("DOUBAO_API_KEY"),
        )
        
        response = client.chat.completions.create(
            model=DOUBAO_API_CONFIG["model_name"],
            messages=conversations,
            temperature=DOUBAO_API_CONFIG["temperature"],
            top_p=DOUBAO_API_CONFIG["top_p"],
            max_tokens=DOUBAO_API_CONFIG["max_tokens"],
            thinking={"type": "disabled" if DOUBAO_API_CONFIG["thinking_disabled"] else "enabled"},
            stream=True
        )
        
        content = ""
        for chunk in response:
            reasoning_content = chunk.choices[0].delta.reasoning_content
            delta_content = chunk.choices[0].delta.content
            if reasoning_content:
                content += reasoning_content
            if delta_content:
                content += delta_content
        
        return content

    async def process_images_batch(self, image_folder_path: str, batch_size: int = None) -> List[ProcessingResult]:
        """æ‰¹é‡å¤„ç†å›¾ç‰‡æ–‡ä»¶å¤¹"""
        
        # ä½¿ç”¨é…ç½®çš„æ‰¹æ¬¡å¤§å°
        if batch_size is None:
            batch_size = PERFORMANCE_CONFIG["batch_size"]
        
        # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶ï¼ˆä¿®å¤é‡å¤è®¡æ•°é—®é¢˜ï¼‰
        image_paths = []
        folder_path = Path(image_folder_path)
        
        # æ”¯æŒçš„å›¾ç‰‡æ‰©å±•å
        supported_extensions = FILE_CONFIG["supported_extensions"]
        
        # éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        for file_path in folder_path.iterdir():
            if file_path.is_file():
                # å°†æ‰©å±•åè½¬ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒï¼Œé¿å…é‡å¤
                file_ext = file_path.suffix.lower()
                if file_ext in supported_extensions:
                    image_paths.append(file_path)
        
        # å»é‡ï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰å¹¶æ’åº
        image_paths = list(set(image_paths))
        image_paths.sort()
        
        self.total_count = len(image_paths)
        self.start_time = time.time()
        
        logger.info(f"ğŸ“ æ–‡ä»¶å¤¹æ‰«æå®Œæˆ: å‘ç° {self.total_count} å¼ å›¾ç‰‡")
        
        # æ‰“å°æ–‡ä»¶æ‰©å±•åç»Ÿè®¡
        ext_count = {}
        for path in image_paths:
            ext = path.suffix.lower()
            ext_count[ext] = ext_count.get(ext, 0) + 1
        
        logger.info(f"ğŸ“Š æ–‡ä»¶ç±»å‹åˆ†å¸ƒ: {dict(ext_count)}")
        
        if self.total_count == 0:
            logger.warning(f"âŒ æ–‡ä»¶å¤¹ {image_folder_path} ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
            return []
        
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç† {self.total_count} å¼ å›¾ç‰‡ï¼Œç›®æ ‡: 1åˆ†é’Ÿå†…å®Œæˆ")
        logger.info(f"âš™ï¸  é…ç½®: æœ€å¤§å¹¶å‘æ•°={self.max_concurrent_requests}, æ‰¹æ¬¡å¤§å°={batch_size}")
        
        # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
        tasks = []
        for image_path in image_paths:
            output_path = image_path.with_suffix('.md')
            task = self.process_single_image_async(str(image_path), str(output_path))
            tasks.append(task)
        
        # åˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜è¿‡è½½
        all_results = []
        for i in range(0, len(tasks), batch_size):
            batch_tasks = tasks[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (len(tasks) + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} (åŒ…å« {len(batch_tasks)} ä¸ªä»»åŠ¡)")
            
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            # å¤„ç†å¼‚å¸¸ç»“æœ
            for j, result in enumerate(batch_results):
                if isinstance(result, Exception):
                    image_path = str(image_paths[i + j])
                    logger.error(f"âŒ æ‰¹æ¬¡å¤„ç†å¼‚å¸¸ {Path(image_path).name}: {result}")
                    all_results.append(ProcessingResult(
                        image_path=image_path,
                        success=False,
                        error=str(result)
                    ))
                else:
                    all_results.append(result)
        
        return all_results

    def print_summary(self, results: List[ProcessingResult]):
        """æ‰“å°å¤„ç†æ‘˜è¦"""
        total_time = time.time() - self.start_time
        successful = sum(1 for r in results if r.success)
        failed = len(results) - successful
        
        avg_processing_time = sum(r.processing_time for r in results if r.success) / max(successful, 1)
        images_per_minute = (successful / total_time) * 60 if total_time > 0 else 0
        target_speed = PERFORMANCE_CONFIG["target_speed"]
        
        logger.info("=" * 60)
        logger.info("ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡:")
        logger.info(f"   æ€»å›¾ç‰‡æ•°: {len(results)}")
        logger.info(f"   æˆåŠŸå¤„ç†: {successful}")
        logger.info(f"   å¤„ç†å¤±è´¥: {failed}")
        logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
        logger.info(f"   å¹³å‡æ¯å¼ : {avg_processing_time:.2f}ç§’")
        logger.info(f"   å¤„ç†é€Ÿåº¦: {images_per_minute:.1f}å¼ /åˆ†é’Ÿ")
        logger.info(f"   ç›®æ ‡è¾¾æˆ: {'âœ… æ˜¯' if images_per_minute >= target_speed else 'âŒ å¦'} (ç›®æ ‡: {target_speed}å¼ /åˆ†é’Ÿ)")
        logger.info("=" * 60)
        
        if failed > 0:
            logger.info("âŒ å¤±è´¥çš„æ–‡ä»¶:")
            for result in results:
                if not result.success:
                    logger.error(f"   {Path(result.image_path).name}: {result.error}")

async def main():
    """ä¸»å‡½æ•°"""
    # ä½¿ç”¨é…ç½®å‚æ•°
    image_folder_path = FILE_CONFIG["default_image_folder"]
    max_concurrent_requests = PERFORMANCE_CONFIG["max_concurrent_requests"]
    batch_size = PERFORMANCE_CONFIG["batch_size"]
    
    # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(image_folder_path):
        logger.error(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {image_folder_path}")
        return
    
    # ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    async with FastImageProcessor(
        max_concurrent_requests=max_concurrent_requests,
        max_retries=3
    ) as processor:
        
        # æ‰¹é‡å¤„ç†å›¾ç‰‡
        results = await processor.process_images_batch(
            image_folder_path=image_folder_path,
            batch_size=batch_size
        )
        
        # æ‰“å°æ‘˜è¦
        processor.print_summary(results)
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        await save_processing_report(results)

async def save_processing_report(results: List[ProcessingResult]):
    """ä¿å­˜å¤„ç†æŠ¥å‘Š"""
    report_path = FILE_CONFIG["report_file_path"]
    
    report_content = "# å›¾ç‰‡å¤„ç†æŠ¥å‘Š\n\n"
    report_content += f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    
    successful_results = [r for r in results if r.success]
    failed_results = [r for r in results if not r.success]
    
    report_content += f"## ç»Ÿè®¡ä¿¡æ¯\n"
    report_content += f"- æ€»æ•°: {len(results)}\n"
    report_content += f"- æˆåŠŸ: {len(successful_results)}\n"
    report_content += f"- å¤±è´¥: {len(failed_results)}\n\n"
    
    if successful_results:
        report_content += "## æˆåŠŸå¤„ç†çš„æ–‡ä»¶\n"
        for result in successful_results:
            report_content += f"- {Path(result.image_path).name} ({result.processing_time:.2f}s)\n"
        report_content += "\n"
    
    if failed_results:
        report_content += "## å¤„ç†å¤±è´¥çš„æ–‡ä»¶\n"
        for result in failed_results:
            report_content += f"- {Path(result.image_path).name}: {result.error}\n"
        report_content += "\n"
    
    async with aiofiles.open(report_path, "w", encoding="utf-8") as f:
        await f.write(report_content)
    
    logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

# åŒæ­¥ç‰ˆæœ¬çš„å¿«é€Ÿå¤„ç†å™¨ï¼ˆå¦‚æœä¸æƒ³ä½¿ç”¨å¼‚æ­¥ï¼‰
class SyncFastImageProcessor:
    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or PERFORMANCE_CONFIG["max_workers_sync"]
        self.processed_count = 0
        self.total_count = 0
        self.start_time = 0
    
    def get_image_base64(self, image_path: str) -> str:
        """è·å–å›¾ç‰‡çš„base64ç¼–ç """
        with open(image_path, "rb") as image_file:
            image_data = image_file.read()
        
        mime_type, _ = mimetypes.guess_type(image_path)
        if mime_type is None:
            mime_type = "application/octet-stream"
        
        image_base64 = base64.b64encode(image_data).decode('utf-8')
        return f"data:{mime_type};base64,{image_base64}"

    def call_doubao_api(self, image_base64: str) -> str:
        """è°ƒç”¨è±†åŒ…API"""
        conversations = [
            {"role": "system", "content": "ä½ å¿…é¡»ç²¾å‡†æå–PDFå›¾ç‰‡çš„å†…å®¹ã€‚"}, 
            {"role": "user", "content": [
                {"type": "text", "text": DOUBAO_VLM_PROMPT},
                {"type": "image_url", "image_url": {"url": image_base64}}
            ]}
        ]
        
        client = Ark(
            base_url=DOUBAO_API_CONFIG["base_url"],
            api_key=os.environ.get("DOUBAO_API_KEY"),
        )
        
        response = client.chat.completions.create(
            model=DOUBAO_API_CONFIG["model_name"],
            messages=conversations,
            temperature=DOUBAO_API_CONFIG["temperature"],
            top_p=DOUBAO_API_CONFIG["top_p"],
            max_tokens=DOUBAO_API_CONFIG["max_tokens"],
            thinking={"type": "disabled" if DOUBAO_API_CONFIG["thinking_disabled"] else "enabled"},
            stream=True
        )
        
        content = ""
        for chunk in response:
            reasoning_content = chunk.choices[0].delta.reasoning_content
            delta_content = chunk.choices[0].delta.content
            if reasoning_content:
                content += reasoning_content
            if delta_content:
                content += delta_content
        
        return content

    def process_single_image(self, image_path: str, output_path: str) -> ProcessingResult:
        """å¤„ç†å•ä¸ªå›¾ç‰‡"""
        start_time = time.time()
        
        try:
            image_base64 = self.get_image_base64(image_path)
            content = self.call_doubao_api(image_base64)
            
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(content)
            
            processing_time = time.time() - start_time
            self.processed_count += 1
            
            # è®¡ç®—è¿›åº¦
            progress = (self.processed_count / self.total_count) * 100
            elapsed = time.time() - self.start_time
            avg_time = elapsed / self.processed_count
            eta = (self.total_count - self.processed_count) * avg_time
            
            logger.info(
                f"âœ… å®Œæˆ [{self.processed_count}/{self.total_count}] "
                f"({progress:.1f}%) {Path(image_path).name} "
                f"è€—æ—¶: {processing_time:.2f}s, ETA: {eta:.1f}s"
            )
            
            return ProcessingResult(
                image_path=image_path,
                success=True,
                content=content,
                processing_time=processing_time
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = str(e)
            logger.error(f"âŒ {Path(image_path).name}: {error_msg}")
            
            return ProcessingResult(
                image_path=image_path,
                success=False,
                error=error_msg,
                processing_time=processing_time
            )

    def process_images_concurrent(self, image_folder_path: str) -> List[ProcessingResult]:
        """ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†å›¾ç‰‡"""
        
        # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶ï¼ˆä¿®å¤é‡å¤è®¡æ•°é—®é¢˜ï¼‰
        image_paths = []
        folder_path = Path(image_folder_path)
        
                # æ”¯æŒçš„å›¾ç‰‡æ‰©å±•å
        supported_extensions = FILE_CONFIG["supported_extensions"]

        # éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        for file_path in folder_path.iterdir():
            if file_path.is_file():
                # å°†æ‰©å±•åè½¬ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒï¼Œé¿å…é‡å¤
                file_ext = file_path.suffix.lower()
                if file_ext in supported_extensions:
                    image_paths.append(file_path)
        
        # å»é‡ï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰å¹¶æ’åº
        image_paths = list(set(image_paths))
        image_paths.sort()
        
        self.total_count = len(image_paths)
        self.start_time = time.time()
        
        logger.info(f"ğŸ“ æ–‡ä»¶å¤¹æ‰«æå®Œæˆ: å‘ç° {self.total_count} å¼ å›¾ç‰‡")
        
        # æ‰“å°æ–‡ä»¶æ‰©å±•åç»Ÿè®¡
        ext_count = {}
        for path in image_paths:
            ext = path.suffix.lower()
            ext_count[ext] = ext_count.get(ext, 0) + 1
        
        logger.info(f"ğŸ“Š æ–‡ä»¶ç±»å‹åˆ†å¸ƒ: {dict(ext_count)}")
        
        if self.total_count == 0:
            logger.warning(f"âŒ æ–‡ä»¶å¤¹ {image_folder_path} ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
            return []
        
        logger.info(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç† {self.total_count} å¼ å›¾ç‰‡")
        logger.info(f"âš™ï¸  é…ç½®: æœ€å¤§å¹¶å‘æ•°={self.max_workers}")
        
        # å‡†å¤‡ä»»åŠ¡å‚æ•°
        task_args = []
        for image_path in image_paths:
            output_path = image_path.with_suffix('.md')
            task_args.append((str(image_path), str(output_path)))
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            results = list(executor.map(
                lambda args: self.process_single_image(*args), 
                task_args
            ))
        
        return results

def run_sync_version():
    """è¿è¡ŒåŒæ­¥ç‰ˆæœ¬"""
    image_folder_path = FILE_CONFIG["default_image_folder"]
    
    processor = SyncFastImageProcessor(max_workers=PERFORMANCE_CONFIG["max_workers_sync"])
    results = processor.process_images_concurrent(image_folder_path)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    total_time = time.time() - processor.start_time
    successful = sum(1 for r in results if r.success)
    failed = len(results) - successful
    images_per_minute = (successful / total_time) * 60 if total_time > 0 else 0
    
    logger.info("=" * 60)
    logger.info("ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡:")
    logger.info(f"   æˆåŠŸ: {successful}, å¤±è´¥: {failed}")
    logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
    logger.info(f"   å¤„ç†é€Ÿåº¦: {images_per_minute:.1f}å¼ /åˆ†é’Ÿ")
    logger.info(f"   ç›®æ ‡è¾¾æˆ: {'âœ… æ˜¯' if images_per_minute >= PERFORMANCE_CONFIG["target_speed"] else 'âŒ å¦'}")
    logger.info("=" * 60)

# ============= Jupyter ç¯å¢ƒé€‚é…ä»£ç  =============

def is_jupyter_environment():
    """æ£€æµ‹æ˜¯å¦åœ¨Jupyterç¯å¢ƒä¸­è¿è¡Œ"""
    try:
        from IPython import get_ipython
        return get_ipython() is not None
    except ImportError:
        return False

def setup_jupyter_asyncio():
    """ä¸ºJupyterç¯å¢ƒé…ç½®asyncio"""
    try:
        import nest_asyncio
        nest_asyncio.apply()
        logger.info("âœ… Jupyter asyncioç¯å¢ƒé…ç½®å®Œæˆ")
    except ImportError:
        logger.warning("âš ï¸  éœ€è¦å®‰è£… nest_asyncio: pip install nest_asyncio")
        raise ImportError("è¯·è¿è¡Œ: pip install nest_asyncio")

async def run_in_jupyter(image_folder_path: str = None, 
                        max_concurrent: int = None, 
                        batch_size: int = None):
    """
    åœ¨Jupyterä¸­è¿è¡Œçš„ä¸»å‡½æ•°
    
    Args:
        image_folder_path: å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
        max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
        batch_size: æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
    """
    # ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼
    if image_folder_path is None:
        image_folder_path = FILE_CONFIG["default_image_folder"]
    if max_concurrent is None:
        max_concurrent = PERFORMANCE_CONFIG["max_concurrent_requests"]
    if batch_size is None:
        batch_size = PERFORMANCE_CONFIG["batch_size"]
    
    logger.info("ğŸ”§ Jupyterç¯å¢ƒå¯åŠ¨ä¸­...")
    
    # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(image_folder_path):
        logger.error(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {image_folder_path}")
        return None
    
    # ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    async with FastImageProcessor(
        max_concurrent_requests=max_concurrent,
        max_retries=3
    ) as processor:
        
        # æ‰¹é‡å¤„ç†å›¾ç‰‡
        results = await processor.process_images_batch(
            image_folder_path=image_folder_path,
            batch_size=batch_size
        )
        
        # æ‰“å°æ‘˜è¦
        processor.print_summary(results)
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        await save_processing_report(results)
        
        return results

# ============= å®‰å…¨çš„è¿è¡Œæ–¹å¼ =============

def run_main_safely():
    """ã€æ¶æ„è®¾è®¡ã€‘å®‰å…¨è¿è¡Œä¸»å‡½æ•°ï¼Œé¿å…ä¸Flaskå†²çª"""
    # æ£€æµ‹Flaskä¸Šä¸‹æ–‡ï¼Œé¿å…å†²çª
    if _FLASK_CONTEXT:
        logger.info("ğŸ”§ æ£€æµ‹åˆ°Flaskç¯å¢ƒï¼Œè·³è¿‡è‡ªåŠ¨è¿è¡Œä»¥é¿å…å†²çª")
        logger.info("ğŸ’¡ å¦‚éœ€è¿è¡Œï¼Œè¯·ä½¿ç”¨: python -m utils.agent_tool_vlm_img2txt_doubao")
        return
    
    if is_jupyter_environment():
        print("âš ï¸  æ£€æµ‹åˆ°Jupyterç¯å¢ƒï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹ä»£ç è¿è¡Œï¼š")
        print("""
# 1. é¦–å…ˆè¿è¡Œè¿™ä¸ªcellå®‰è£…ä¾èµ–ï¼š
!pip install nest_asyncio aiohttp aiofiles

# 2. ç„¶ååœ¨æ–°çš„cellä¸­è¿è¡Œï¼š
setup_jupyter_asyncio()

# 3. æœ€åè¿è¡Œå¤„ç†ä»»åŠ¡ï¼ˆä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼ï¼‰ï¼š
results = await run_in_jupyter()  # ä½¿ç”¨é»˜è®¤é…ç½®

# æˆ–è€…è‡ªå®šä¹‰å‚æ•°ï¼š
# results = await run_in_jupyter(
#     image_folder_path="your_custom_folder",  # å¯é€‰ï¼šè‡ªå®šä¹‰æ–‡ä»¶å¤¹è·¯å¾„
#     max_concurrent=25,                       # å¯é€‰ï¼šè‡ªå®šä¹‰å¹¶å‘æ•°
#     batch_size=50                            # å¯é€‰ï¼šè‡ªå®šä¹‰æ‰¹æ¬¡å¤§å°
# )
        """)
    else:
        # æ™®é€šPythonç¯å¢ƒè¿è¡Œ
        try:
            asyncio.run(main())
        except KeyboardInterrupt:
            logger.info("â¹ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†")
        except Exception as e:
            logger.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")

if __name__ == "__main__":
    run_main_safely()

# ============= Jupyterç®€åŒ–è¿è¡Œå‡½æ•° =============

def quick_start_jupyter(folder_path: str = None):
    """
    Jupyterç¯å¢ƒä¸€é”®å¯åŠ¨å‡½æ•°
    
    Usage in Jupyter:
        quick_start_jupyter("your_image_folder_path")
    """
    if not is_jupyter_environment():
        logger.error("âŒ æ­¤å‡½æ•°ä»…é€‚ç”¨äºJupyterç¯å¢ƒ")
        return
    
    # ä½¿ç”¨é…ç½®çš„é»˜è®¤å€¼
    if folder_path is None:
        folder_path = FILE_CONFIG["default_image_folder"]
    
    try:
        setup_jupyter_asyncio()
        
        # åˆ›å»ºå¹¶è¿è¡Œä»»åŠ¡
        import asyncio
        loop = asyncio.get_event_loop()
        
        async def _run():
            return await run_in_jupyter(folder_path)
        
        return loop.create_task(_run())
        
    except Exception as e:
        logger.error(f"âŒ Jupyterå¯åŠ¨å¤±è´¥: {e}")
        return None

# ============= åŸç‰ˆæœ¬è¿è¡Œæ–¹å¼ä¿ç•™ =============

def run_sync_version_main():
    """è¿è¡ŒåŒæ­¥ç‰ˆæœ¬çš„ä¸»å‡½æ•°"""
    image_folder_path = FILE_CONFIG["default_image_folder"]
    
    processor = SyncFastImageProcessor(max_workers=PERFORMANCE_CONFIG["max_workers_sync"])
    results = processor.process_images_concurrent(image_folder_path)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    total_time = time.time() - processor.start_time
    successful = sum(1 for r in results if r.success)
    failed = len(results) - successful
    images_per_minute = (successful / total_time) * 60 if total_time > 0 else 0
    
    logger.info("=" * 60)
    logger.info("ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡:")
    logger.info(f"   æˆåŠŸ: {successful}, å¤±è´¥: {failed}")
    logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
    logger.info(f"   å¤„ç†é€Ÿåº¦: {images_per_minute:.1f}å¼ /åˆ†é’Ÿ")
    logger.info(f"   ç›®æ ‡è¾¾æˆ: {'âœ… æ˜¯' if images_per_minute >= PERFORMANCE_CONFIG["target_speed"] else 'âŒ å¦'}")
    logger.info("=" * 60)

