# type: ignore

"""
PDF VLMå¤„ç†å™¨ - æé€Ÿä¼˜åŒ–ç‰ˆæœ¬ V5
ä½¿ç”¨æ··åˆä¼˜åŒ–ç­–ç•¥å®ç°æè‡´æ€§èƒ½ï¼šæ¿€è¿›å¹¶å‘+æ™ºèƒ½è´Ÿè½½å‡è¡¡+é¢„å¤„ç†ç¼“å­˜
è·¯å¾„ï¼šagent/utils/pdf_vlm_processor_v5_turbo.py
"""

import os
import sys
import time
import threading
import queue
import pickle
import base64
import asyncio
import aiohttp
import json
import random
import hashlib
import psutil
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Any, Set
from pdf2image import convert_from_path
from PIL import Image
import multiprocessing as mp
from functools import partial
import logging
from dataclasses import dataclass, field
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor
import io

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class ProcessingResult:
    """å¤„ç†ç»“æœæ•°æ®ç±»"""
    page_num: int
    content: str
    success: bool
    error: Optional[str] = None
    processing_time: float = 0.0
    retry_count: int = 0
    api_response_time: float = 0.0
    queue_wait_time: float = 0.0

@dataclass
class VLMTask:
    """VLMå¤„ç†ä»»åŠ¡"""
    page_num: int
    image_path: str
    image_hash: str
    api_key: str
    model: str
    priority: int = 0
    created_time: float = field(default_factory=time.time)
    retry_count: int = 0

@dataclass
class BatchMetrics:
    """æ‰¹æ¬¡æ€§èƒ½æŒ‡æ ‡"""
    batch_id: int
    task_count: int
    success_count: int = 0
    total_time: float = 0.0
    avg_response_time: float = 0.0
    throughput: float = 0.0

class ImageCache:
    """å›¾ç‰‡ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, max_size: int = 100):
        self.cache: Dict[str, str] = {}
        self.access_times: Dict[str, float] = {}
        self.max_size = max_size
        self.lock = threading.Lock()
    
    def get_image_hash(self, image_path: str) -> str:
        """è®¡ç®—å›¾ç‰‡å“ˆå¸Œå€¼"""
        with open(image_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    def get_base64(self, image_path: str, image_hash: str) -> str:
        """è·å–å›¾ç‰‡çš„Base64ç¼–ç ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        with self.lock:
            if image_hash in self.cache:
                self.access_times[image_hash] = time.time()
                return self.cache[image_hash]
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œè¯»å–å¹¶ç¼–ç å›¾ç‰‡
            try:
                with open(image_path, "rb") as image_file:
                    image_base64 = base64.b64encode(image_file.read()).decode('utf-8')
                    image_data = f"data:image/png;base64,{image_base64}"
                
                # æ·»åŠ åˆ°ç¼“å­˜
                if len(self.cache) >= self.max_size:
                    self._evict_oldest()
                
                self.cache[image_hash] = image_data
                self.access_times[image_hash] = time.time()
                return image_data
                
            except Exception as e:
                logger.error(f"è¯»å–å›¾ç‰‡å¤±è´¥ {image_path}: {str(e)}")
                raise
    
    def _evict_oldest(self):
        """ç§»é™¤æœ€ä¹…æœªä½¿ç”¨çš„ç¼“å­˜é¡¹"""
        if not self.access_times:
            return
        
        oldest_hash = min(self.access_times.keys(), 
                         key=lambda k: self.access_times[k])
        del self.cache[oldest_hash]
        del self.access_times[oldest_hash]

class AdaptiveConcurrencyManager:
    """è‡ªé€‚åº”å¹¶å‘ç®¡ç†å™¨"""
    
    def __init__(self, initial_concurrency: int = 20, max_concurrency: int = 50):
        self.current_concurrency = initial_concurrency
        self.max_concurrency = max_concurrency
        self.min_concurrency = 5
        self.success_rate_window = []
        self.response_time_window = []
        self.window_size = 20
        self.lock = threading.Lock()
        
    def update_metrics(self, success: bool, response_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        with self.lock:
            self.success_rate_window.append(1 if success else 0)
            self.response_time_window.append(response_time)
            
            if len(self.success_rate_window) > self.window_size:
                self.success_rate_window.pop(0)
            if len(self.response_time_window) > self.window_size:
                self.response_time_window.pop(0)
            
            # è‡ªé€‚åº”è°ƒæ•´å¹¶å‘æ•°
            self._adjust_concurrency()
    
    def _adjust_concurrency(self):
        """è‡ªé€‚åº”è°ƒæ•´å¹¶å‘æ•°"""
        if len(self.success_rate_window) < 10:
            return
        
        success_rate = sum(self.success_rate_window) / len(self.success_rate_window)
        avg_response_time = sum(self.response_time_window) / len(self.response_time_window)
        
        # æ ¹æ®æˆåŠŸç‡å’Œå“åº”æ—¶é—´è°ƒæ•´å¹¶å‘æ•°
        if success_rate > 0.95 and avg_response_time < 20:
            # é«˜æˆåŠŸç‡ï¼Œä½å»¶è¿Ÿï¼šå¢åŠ å¹¶å‘
            new_concurrency = min(self.current_concurrency + 2, self.max_concurrency)
        elif success_rate < 0.8 or avg_response_time > 40:
            # ä½æˆåŠŸç‡æˆ–é«˜å»¶è¿Ÿï¼šå‡å°‘å¹¶å‘
            new_concurrency = max(self.current_concurrency - 3, self.min_concurrency)
        else:
            new_concurrency = self.current_concurrency
        
        if new_concurrency != self.current_concurrency:
            logger.info(f"è°ƒæ•´å¹¶å‘æ•°: {self.current_concurrency} -> {new_concurrency} "
                       f"(æˆåŠŸç‡: {success_rate:.3f}, å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.1f}s)")
            self.current_concurrency = new_concurrency
    
    def get_concurrency(self) -> int:
        """è·å–å½“å‰å»ºè®®çš„å¹¶å‘æ•°"""
        return self.current_concurrency

async def _ultra_fast_vlm_request(session: aiohttp.ClientSession, 
                                 task: VLMTask, 
                                 image_cache: ImageCache,
                                 concurrency_manager: AdaptiveConcurrencyManager,
                                 semaphore: asyncio.Semaphore) -> ProcessingResult:
    """
    è¶…å¿«é€ŸVLMå•ä¸ªè¯·æ±‚ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    """
    async with semaphore:  # é™åˆ¶å¹¶å‘æ•°
        queue_start = time.time()
        request_start = time.time()
        
        try:
            # ä»ç¼“å­˜è·å–å›¾ç‰‡æ•°æ®
            image_data = image_cache.get_base64(task.image_path, task.image_hash)
            
            # ä¼˜åŒ–çš„è¯·æ±‚é…ç½®
            question = "è¯·é˜…è¯»æˆ‘ä¸Šä¼ çš„pdfæ–‡ä»¶ï¼Œä½¿ç”¨markdownæ ¼å¼è¿”å›æ‰€æœ‰çš„ä¿¡æ¯ã€‚å¦‚æœæœ‰å›¾ç‰‡ï¼Œéœ€è¦ä½ ç”¨ä¸€ä¸ªmarkdownæ ‡é¢˜+æ–‡å­—æè¿°ï¼Œæ ‡é¢˜ä¸ºå›¾ç‰‡çš„æ ‡é¢˜ï¼Œæ–‡å­—æè¿°éœ€è¦è¯¦ç»†å…¨é¢åœ°ä»‹ç»è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚æ³¨æ„ï¼šä½ çš„è¾“å‡ºå¿…é¡»ä¸åŸæ–‡çš„è¯­ç§ä¸€è‡´ã€‚æˆ‘æä¾›çš„å›¾ç‰‡æ˜¯è‹±æ–‡ï¼Œä½ çš„è¾“å‡ºä¹Ÿå¿…é¡»æ˜¯è‹±æ–‡ã€‚"
            
            payload = {
                "model": task.model,
                "messages": [
                    {"role": "system", "content": "ä½ å¿…é¡»ç²¾å‡†å¿«é€Ÿæå–PDFå›¾ç‰‡çš„å†…å®¹ã€‚"}, 
                    {"role": "user", "content": [
                        {"type": "text", "text": question},
                        {"type": "image_url", "image_url": {"url": image_data}}
                    ]}
                ],
                "temperature": 0.7,  # é™ä½æ¸©åº¦ä»¥æé«˜é€Ÿåº¦
                "top_p": 0.8,
                "max_tokens": 8192,  # é€‚å½“å‡å°‘max_tokens
                "thinking": {"type": "disabled"},
                "stream": False
            }
            
            headers = {
                "Authorization": f"Bearer {task.api_key}",
                "Content-Type": "application/json"
            }
            
            queue_wait_time = time.time() - queue_start
            api_start = time.time()
            
            # åŠ¨æ€è¶…æ—¶æ—¶é—´ï¼ˆåŸºäºå†å²è¡¨ç°ï¼‰
            base_timeout = 25  # å¤§å¹…ç¼©çŸ­åŸºç¡€è¶…æ—¶æ—¶é—´
            timeout_seconds = base_timeout + (task.retry_count * 10)
            
            # å‘é€è¯·æ±‚
            async with session.post(
                "https://ark.cn-beijing.volces.com/api/v3/chat/completions",
                json=payload,
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=timeout_seconds, connect=10, sock_read=20)
            ) as response:
                
                api_response_time = time.time() - api_start
                
                if response.status == 200:
                    result = await response.json()
                    content = result['choices'][0]['message']['content']
                    
                    processing_time = time.time() - request_start
                    
                    # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
                    concurrency_manager.update_metrics(True, api_response_time)
                    
                    logger.debug(f"é¡µé¢ {task.page_num} æˆåŠŸï¼Œè€—æ—¶ {processing_time:.2f}s")
                    
                    return ProcessingResult(
                        page_num=task.page_num,
                        content=content,
                        success=True,
                        processing_time=processing_time,
                        retry_count=task.retry_count,
                        api_response_time=api_response_time,
                        queue_wait_time=queue_wait_time
                    )
                else:
                    error_text = await response.text()
                    raise Exception(f"APIé”™è¯¯ {response.status}: {error_text}")
                    
        except Exception as e:
            processing_time = time.time() - request_start
            api_response_time = time.time() - api_start if 'api_start' in locals() else 0
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            concurrency_manager.update_metrics(False, api_response_time or 30)
            
            logger.warning(f"é¡µé¢ {task.page_num} è¯·æ±‚å¤±è´¥: {str(e)}")
            
            return ProcessingResult(
                page_num=task.page_num,
                content="",
                success=False,
                error=str(e),
                processing_time=processing_time,
                retry_count=task.retry_count,
                api_response_time=api_response_time,
                queue_wait_time=queue_wait_time if 'queue_wait_time' in locals() else 0
            )

async def _turbo_batch_processor(tasks: List[VLMTask], 
                               image_cache: ImageCache,
                               concurrency_manager: AdaptiveConcurrencyManager,
                               batch_id: int) -> List[ProcessingResult]:
    """
    æé€Ÿæ‰¹é‡å¤„ç†å™¨
    """
    start_time = time.time()
    
    # åŠ¨æ€è·å–å¹¶å‘æ•°
    max_concurrent = concurrency_manager.get_concurrency()
    semaphore = asyncio.Semaphore(max_concurrent)
    
    # ä¼˜åŒ–çš„è¿æ¥å™¨é…ç½®
    connector = aiohttp.TCPConnector(
        limit=max_concurrent + 20,  # å¢åŠ è¿æ¥æ± å¤§å°
        limit_per_host=max_concurrent,
        ttl_dns_cache=600,
        use_dns_cache=True,
        keepalive_timeout=120,
        enable_cleanup_closed=True,
        force_close=False,
        auto_decompress=True
    )
    
    # çŸ­è¶…æ—¶é…ç½®ï¼Œå¿«é€Ÿå¤±è´¥
    timeout = aiohttp.ClientTimeout(total=35, connect=5, sock_read=25)
    
    try:
        async with aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            trust_env=True
        ) as session:
            
            # åˆ›å»ºæ‰€æœ‰å¼‚æ­¥ä»»åŠ¡
            async_tasks = [
                _ultra_fast_vlm_request(session, task, image_cache, concurrency_manager, semaphore)
                for task in tasks
            ]
            
            # å¹¶å‘æ‰§è¡Œ
            results = await asyncio.gather(*async_tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            processed_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"æ‰¹æ¬¡ {batch_id} ä»»åŠ¡ {i} å¼‚å¸¸: {str(result)}")
                    processed_results.append(ProcessingResult(
                        page_num=tasks[i].page_num,
                        content="",
                        success=False,
                        error=str(result)
                    ))
                else:
                    processed_results.append(result)
            
            # è®¡ç®—æ‰¹æ¬¡æŒ‡æ ‡
            total_time = time.time() - start_time
            success_count = sum(1 for r in processed_results if r.success)
            
            batch_metrics = BatchMetrics(
                batch_id=batch_id,
                task_count=len(tasks),
                success_count=success_count,
                total_time=total_time,
                avg_response_time=sum(r.api_response_time for r in processed_results if r.success) / max(success_count, 1),
                throughput=success_count / total_time if total_time > 0 else 0
            )
            
            logger.info(f"æ‰¹æ¬¡ {batch_id} å®Œæˆ: {success_count}/{len(tasks)} æˆåŠŸ, "
                       f"è€—æ—¶ {total_time:.2f}s, ååé‡ {batch_metrics.throughput:.2f} ä»»åŠ¡/ç§’")
            
            return processed_results
            
    except Exception as e:
        logger.error(f"æ‰¹æ¬¡ {batch_id} å¤„ç†å¼‚å¸¸: {str(e)}")
        return [ProcessingResult(
            page_num=task.page_num,
            content="",
            success=False,
            error=str(e)
        ) for task in tasks]

def _turbo_batch_worker(args: Tuple[List[VLMTask], int]) -> List[ProcessingResult]:
    """
    æé€Ÿæ‰¹é‡å·¥ä½œè¿›ç¨‹å…¥å£
    """
    tasks, batch_id = args
    
    # é¢„å…ˆè®¡ç®—å›¾ç‰‡ç¼“å­˜
    image_cache = ImageCache(max_size=len(tasks) + 20)
    concurrency_manager = AdaptiveConcurrencyManager(
        initial_concurrency=min(25, len(tasks)),  # æ¿€è¿›çš„åˆå§‹å¹¶å‘
        max_concurrency=40
    )
    
    # åœ¨æ–°è¿›ç¨‹ä¸­åˆ›å»ºäº‹ä»¶å¾ªç¯
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        return loop.run_until_complete(
            _turbo_batch_processor(tasks, image_cache, concurrency_manager, batch_id)
        )
    finally:
        loop.close()

class PDFVLMProcessorV5Turbo:
    """PDF VLMå¤„ç†å™¨ - æé€Ÿä¼˜åŒ–ç‰ˆæœ¬ V5"""
    
    def __init__(self, 
                 pdf_workers: Optional[int] = None,
                 vlm_workers: Optional[int] = None,
                 batch_size: int = 15,  # æ¿€è¿›çš„æ‰¹æ¬¡å¤§å°
                 initial_concurrency: int = 25,  # æ¿€è¿›çš„åˆå§‹å¹¶å‘
                 max_concurrency: int = 40,
                 dpi: int = 150,  # é€‚å½“é™ä½DPIä»¥å‡å°‘æ•°æ®ä¼ è¾“
                 model: str = "doubao-seed-1-6-flash-250615",
                 enable_image_compression: bool = True,
                 cache_size: int = 200):
        """
        åˆå§‹åŒ–æé€ŸPDF VLMå¤„ç†å™¨
        """
        self.dpi = dpi
        self.model = model
        self.batch_size = batch_size
        self.initial_concurrency = initial_concurrency
        self.max_concurrency = max_concurrency
        self.enable_image_compression = enable_image_compression
        
        # æ™ºèƒ½è¿›ç¨‹æ•°é…ç½®
        cpu_count = mp.cpu_count()
        system_memory_gb = psutil.virtual_memory().total / (1024**3)
        
        # æ ¹æ®ç³»ç»Ÿèµ„æºåŠ¨æ€é…ç½®
        self.pdf_workers = pdf_workers or min(cpu_count, 6)
        self.vlm_workers = vlm_workers or min(max(2, cpu_count // 2), 8)
        
        # å…¨å±€ç¼“å­˜å’Œç®¡ç†å™¨
        self.image_cache = ImageCache(max_size=cache_size)
        self.concurrency_manager = AdaptiveConcurrencyManager(
            initial_concurrency=initial_concurrency,
            max_concurrency=max_concurrency
        )
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'total_pages': 0,
            'processed_pages': 0,
            'success_pages': 0,
            'failed_pages': 0,
            'total_api_calls': 0,
            'cache_hits': 0,
            'avg_response_time': 0,
            'peak_concurrency': 0,
            'throughput': 0,
            'start_time': None,
            'end_time': None
        }
        
        logger.info(f"åˆå§‹åŒ–æé€Ÿå¤„ç†å™¨V5 - PDFè¿›ç¨‹: {self.pdf_workers}, VLMè¿›ç¨‹: {self.vlm_workers}")
        logger.info(f"æ‰¹æ¬¡å¤§å°: {self.batch_size}, åˆå§‹å¹¶å‘: {self.initial_concurrency}, æœ€å¤§å¹¶å‘: {self.max_concurrency}")
        logger.info(f"ç³»ç»Ÿä¿¡æ¯: CPUæ ¸å¿ƒ {cpu_count}, å†…å­˜ {system_memory_gb:.1f}GB")
    
    def _optimize_image(self, image_path: str) -> str:
        """ä¼˜åŒ–å›¾ç‰‡å¤§å°ä»¥æå‡ä¼ è¾“é€Ÿåº¦"""
        if not self.enable_image_compression:
            return image_path
        
        try:
            with Image.open(image_path) as img:
                # å¦‚æœå›¾ç‰‡å¤ªå¤§ï¼Œè¿›è¡Œå‹ç¼©
                if img.size[0] > 2000 or img.size[1] > 2000:
                    # è®¡ç®—å‹ç¼©æ¯”ä¾‹
                    max_size = 1800
                    ratio = min(max_size / img.size[0], max_size / img.size[1])
                    new_size = (int(img.size[0] * ratio), int(img.size[1] * ratio))
                    
                    # åˆ›å»ºå‹ç¼©ç‰ˆæœ¬
                    compressed_img = img.resize(new_size, Image.Resampling.LANCZOS)
                    
                    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                    compressed_path = image_path.replace('.png', '_compressed.png')
                    compressed_img.save(compressed_path, 'PNG', optimize=True, compress_level=6)
                    
                    return compressed_path
            
            return image_path
            
        except Exception as e:
            logger.warning(f"å›¾ç‰‡ä¼˜åŒ–å¤±è´¥ {image_path}: {str(e)}")
            return image_path
    
    def convert_pdf_to_images_turbo(self, pdf_path: str, output_dir: str, 
                                   file_prefix: Optional[str] = None) -> List[Tuple[int, str]]:
        """
        æé€ŸPDFè½¬å›¾ç‰‡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        """
        os.makedirs(output_dir, exist_ok=True)
        
        # å¿«é€Ÿè·å–é¡µæ•°
        try:
            import fitz
            doc = fitz.open(pdf_path)
            total_pages = doc.page_count
            doc.close()
        except:
            logger.warning("æ— æ³•ä½¿ç”¨PyMuPDFè·å–é¡µæ•°ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
            total_pages = self._get_page_count_fallback(pdf_path)
        
        if total_pages == 0:
            raise ValueError("æ— æ³•è·å–PDFé¡µæ•°")
        
        self.stats['total_pages'] = total_pages
        logger.info(f"PDFæ€»å…± {total_pages} é¡µï¼Œå¼€å§‹æé€Ÿè½¬æ¢...")
        
        if file_prefix is None:
            file_prefix = Path(pdf_path).stem
        
        # ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°ï¼ˆæ›´å¤§çš„æ‰¹æ¬¡ï¼‰
        batch_size = min(8, max(2, total_pages // self.pdf_workers))
        batches = []
        
        for start in range(1, total_pages + 1, batch_size):
            end = min(start + batch_size - 1, total_pages)
            batches.append((pdf_path, start, end, output_dir, file_prefix, self.dpi))
        
        logger.info(f"ä½¿ç”¨ {self.pdf_workers} è¿›ç¨‹å¤„ç† {len(batches)} ä¸ªæ‰¹æ¬¡")
        
        start_time = time.time()
        all_results = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± è€Œä¸æ˜¯è¿›ç¨‹æ± ï¼ˆPDF2imageåœ¨æŸäº›æƒ…å†µä¸‹çº¿ç¨‹æ›´å¿«ï¼‰
        with ThreadPoolExecutor(max_workers=self.pdf_workers) as executor:
            future_to_batch = {executor.submit(_convert_page_range_worker, batch): batch 
                              for batch in batches}
            
            for future in concurrent.futures.as_completed(future_to_batch):
                try:
                    result = future.result()
                    all_results.extend(result)
                    logger.info(f"PDFè½¬æ¢è¿›åº¦: {len(all_results)}/{total_pages}")
                except Exception as e:
                    logger.error(f"PDFè½¬æ¢æ‰¹æ¬¡å¤±è´¥: {str(e)}")
        
        # å›¾ç‰‡ä¼˜åŒ–
        if self.enable_image_compression:
            logger.info("å¼€å§‹å›¾ç‰‡å‹ç¼©ä¼˜åŒ–...")
            optimized_results = []
            with ThreadPoolExecutor(max_workers=4) as executor:
                futures = {executor.submit(self._optimize_image, img_path): (page_num, img_path) 
                          for page_num, img_path in all_results}
                
                for future in concurrent.futures.as_completed(futures):
                    page_num, original_path = futures[future]
                    optimized_path = future.result()
                    optimized_results.append((page_num, optimized_path))
            
            all_results = optimized_results
        
        all_results.sort(key=lambda x: x[0])
        conversion_time = time.time() - start_time
        
        logger.info(f"PDFè½¬å›¾ç‰‡å®Œæˆ: {len(all_results)} å¼ ï¼Œè€—æ—¶ {conversion_time:.2f}s")
        return all_results
    
    def process_images_with_vlm_turbo(self, image_paths: List[Tuple[int, str]]) -> Dict[int, str]:
        """
        æé€ŸVLMæ‰¹é‡å¤„ç†
        """
        api_key = os.environ.get("DOUBAO_API_KEY")
        if not api_key:
            raise ValueError("è¯·è®¾ç½® DOUBAO_API_KEY ç¯å¢ƒå˜é‡")
        
        logger.info(f"å¼€å§‹æé€ŸVLMå¤„ç†: {len(image_paths)} å¼ å›¾ç‰‡")
        
        # é¢„å¤„ç†ï¼šè®¡ç®—å›¾ç‰‡å“ˆå¸Œå¹¶é¢„ç¼“å­˜
        logger.info("é¢„å¤„ç†å›¾ç‰‡ç¼“å­˜...")
        vlm_tasks = []
        for page_num, image_path in image_paths:
            image_hash = self.image_cache.get_image_hash(image_path)
            task = VLMTask(
                page_num=page_num,
                image_path=image_path,
                image_hash=image_hash,
                api_key=api_key,
                model=self.model,
                priority=page_num  # æŒ‰é¡µç ä¼˜å…ˆçº§
            )
            vlm_tasks.append(task)
        
        # åˆ›å»ºæ‰¹æ¬¡ï¼ˆæ›´å¤§çš„æ‰¹æ¬¡å¤§å°ï¼‰
        batches = []
        for i in range(0, len(vlm_tasks), self.batch_size):
            batch_tasks = vlm_tasks[i:i + self.batch_size]
            batches.append((batch_tasks, i // self.batch_size + 1))
        
        logger.info(f"åˆ›å»º {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹æ¬¡æœ€å¤š {self.batch_size} å¼ å›¾ç‰‡")
        
        start_time = time.time()
        all_results = []
        
        # ä½¿ç”¨è¿›ç¨‹æ± è¿›è¡Œæ‰¹é‡å¤„ç†
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.vlm_workers) as executor:
            future_to_batch = {executor.submit(_turbo_batch_worker, batch): batch 
                              for batch in batches}
            
            completed = 0
            for future in concurrent.futures.as_completed(future_to_batch):
                try:
                    batch_results = future.result()
                    all_results.extend(batch_results)
                    completed += 1
                    
                    # å®æ—¶ç»Ÿè®¡
                    success_count = sum(1 for r in batch_results if r.success)
                    logger.info(f"æ‰¹æ¬¡å®Œæˆ: {completed}/{len(batches)} "
                               f"(æœ¬æ‰¹æ¬¡æˆåŠŸ: {success_count}/{len(batch_results)})")
                    
                except Exception as e:
                    logger.error(f"æ‰¹æ¬¡å¤„ç†å¼‚å¸¸: {str(e)}")
        
        # æ•´ç†ç»“æœ
        results = {}
        success_count = 0
        failed_count = 0
        total_response_time = 0
        
        for result in all_results:
            self.stats['total_api_calls'] += 1
            if result.success:
                results[result.page_num] = result.content
                success_count += 1
                total_response_time += result.api_response_time
            else:
                failed_count += 1
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        processing_time = time.time() - start_time
        self.stats['processed_pages'] = len(all_results)
        self.stats['success_pages'] = success_count
        self.stats['failed_pages'] = failed_count
        self.stats['avg_response_time'] = total_response_time / max(success_count, 1)
        self.stats['throughput'] = success_count / processing_time if processing_time > 0 else 0
        
        logger.info(f"æé€ŸVLMå¤„ç†å®Œæˆ: æˆåŠŸ {success_count}/{len(vlm_tasks)} "
                   f"({success_count/len(vlm_tasks)*100:.1f}%), è€—æ—¶ {processing_time:.2f}s")
        logger.info(f"å¹³å‡å“åº”æ—¶é—´: {self.stats['avg_response_time']:.2f}s, "
                   f"ååé‡: {self.stats['throughput']:.2f} é¡µ/ç§’")
        
        return results
    
    def _get_page_count_fallback(self, pdf_path: str) -> int:
        """å¤‡ç”¨æ–¹æ³•è·å–PDFé¡µæ•°"""
        try:
            # å°è¯•ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾
            left, right = 1, 500
            while left < right:
                mid = (left + right + 1) // 2
                try:
                    convert_from_path(pdf_path, dpi=72, first_page=mid, last_page=mid)
                    left = mid
                except:
                    right = mid - 1
            return left
        except:
            return 0
    
    def process_pdf_to_markdown_turbo(self, pdf_path: str, output_md_path: str, 
                                     temp_image_dir: Optional[str] = None,
                                     cleanup_images: bool = True) -> str:
        """
        å®Œæ•´çš„æé€ŸPDFåˆ°Markdownå¤„ç†æµç¨‹
        """
        self.stats['start_time'] = time.time()
        
        if temp_image_dir is None:
            temp_image_dir = os.path.join(os.path.dirname(output_md_path), "temp_images_turbo")
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæé€ŸPDFè½¬å›¾ç‰‡
            logger.info("ğŸš€ ç¬¬ä¸€æ­¥ï¼šæé€ŸPDFè½¬å›¾ç‰‡")
            image_paths = self.convert_pdf_to_images_turbo(pdf_path, temp_image_dir)
            
            if not image_paths:
                raise ValueError("PDFè½¬å›¾ç‰‡å¤±è´¥")
            
            # ç¬¬äºŒæ­¥ï¼šæé€ŸVLMæ‰¹é‡å¤„ç†
            logger.info("âš¡ ç¬¬äºŒæ­¥ï¼šæé€ŸVLMæ‰¹é‡å¤„ç†")
            page_texts = self.process_images_with_vlm_turbo(image_paths)
            
            if not page_texts:
                raise ValueError("VLMå¤„ç†å®Œå…¨å¤±è´¥")
            
            # ç¬¬ä¸‰æ­¥ï¼šæ™ºèƒ½æ–‡æœ¬åˆå¹¶
            logger.info("ğŸ“ ç¬¬ä¸‰æ­¥ï¼šæ™ºèƒ½æ–‡æœ¬åˆå¹¶")
            combined_text = self._smart_combine_texts(page_texts)
            
            # ä¿å­˜ç»“æœ
            os.makedirs(os.path.dirname(output_md_path), exist_ok=True)
            with open(output_md_path, "w", encoding="utf-8") as f:
                f.write(combined_text)
            
            self.stats['end_time'] = time.time()
            self._print_turbo_stats(output_md_path, combined_text)
            
            return combined_text
            
        except Exception as e:
            logger.error(f"æé€Ÿå¤„ç†å¤±è´¥: {str(e)}")
            raise
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if cleanup_images and temp_image_dir and os.path.exists(temp_image_dir):
                try:
                    import shutil
                    shutil.rmtree(temp_image_dir)
                    logger.info(f"å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_image_dir}")
                except Exception as e:
                    logger.warning(f"æ¸…ç†å¤±è´¥: {str(e)}")
    
    def _smart_combine_texts(self, page_texts: Dict[int, str]) -> str:
        """æ™ºèƒ½åˆå¹¶æ–‡æœ¬"""
        if not page_texts:
            return ""
        
        # æŒ‰é¡µç æ’åº
        sorted_pages = sorted(page_texts.keys())
        combined_parts = []
        
        # æ·»åŠ æ–‡æ¡£å¤´éƒ¨
        combined_parts.append("# PDFæ–‡æ¡£å†…å®¹\n\n")
        combined_parts.append(f"*æ–‡æ¡£å¤„ç†æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}*\n\n")
        combined_parts.append(f"*æˆåŠŸå¤„ç†é¡µæ•°: {len(page_texts)}/{self.stats['total_pages']}*\n\n")
        combined_parts.append("---\n\n")
        
        # åˆå¹¶é¡µé¢å†…å®¹
        for page_num in sorted_pages:
            text = page_texts[page_num].strip()
            if text:
                combined_parts.append(f"## ç¬¬ {page_num} é¡µ\n\n{text}\n\n")
        
        # æ·»åŠ ç¼ºå¤±é¡µé¢æŠ¥å‘Š
        all_pages = set(range(1, self.stats['total_pages'] + 1))
        missing_pages = all_pages - set(page_texts.keys())
        if missing_pages:
            combined_parts.append("---\n\n")
            combined_parts.append("## å¤„ç†æŠ¥å‘Š\n\n")
            combined_parts.append(f"**ç¼ºå¤±é¡µé¢**: {sorted(missing_pages)}\n\n")
        
        return "".join(combined_parts)
    
    def _print_turbo_stats(self, output_path: str, content: str):
        """è¾“å‡ºæé€Ÿç‰ˆæ€§èƒ½ç»Ÿè®¡"""
        total_time = self.stats['end_time'] - self.stats['start_time']
        
        logger.info("=" * 70)
        logger.info("ğŸ† æé€Ÿå¤„ç†å®Œæˆ - æ€§èƒ½ç»Ÿè®¡ï¼ˆV5 Turboç‰ˆæœ¬ï¼‰")
        logger.info("=" * 70)
        logger.info(f"ğŸ“Š å¤„ç†æ¦‚è§ˆ:")
        logger.info(f"   æ€»é¡µæ•°: {self.stats['total_pages']}")
        logger.info(f"   æˆåŠŸé¡µæ•°: {self.stats['success_pages']}")
        logger.info(f"   å¤±è´¥é¡µæ•°: {self.stats['failed_pages']}")
        logger.info(f"   æˆåŠŸç‡: {self.stats['success_pages']/self.stats['total_pages']*100:.1f}%")
        logger.info(f"")
        logger.info(f"âš¡ æ€§èƒ½æŒ‡æ ‡:")
        logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}s")
        logger.info(f"   å¹³å‡æ¯é¡µ: {total_time/self.stats['total_pages']:.2f}s")
        logger.info(f"   å¤„ç†ååé‡: {self.stats['throughput']:.2f} é¡µ/ç§’")
        logger.info(f"   å¹³å‡APIå“åº”: {self.stats['avg_response_time']:.2f}s")
        logger.info(f"   æ€»APIè°ƒç”¨: {self.stats['total_api_calls']}")
        logger.info(f"")
        logger.info(f"ğŸ“ è¾“å‡ºä¿¡æ¯:")
        logger.info(f"   æ–‡ä»¶è·¯å¾„: {output_path}")
        logger.info(f"   æ–‡ä»¶å¤§å°: {len(content):,} å­—ç¬¦")
        logger.info(f"   å¹³å‡æ¯é¡µå­—ç¬¦æ•°: {len(content)//max(self.stats['success_pages'], 1):,}")

# ä¾¿æ·å‡½æ•°
def convert_pdf_to_markdown_v5_turbo(pdf_path: str, 
                                    output_md_path: str,
                                    dpi: int = 150,
                                    batch_size: int = 15,
                                    initial_concurrency: int = 25,
                                    max_concurrency: int = 40,
                                    pdf_workers: Optional[int] = None,
                                    vlm_workers: Optional[int] = None,
                                    model: str = "doubao-seed-1-6-flash-250615",
                                    enable_compression: bool = True,
                                    cleanup_images: bool = True) -> str:
    """
    æé€ŸPDFåˆ°Markdownè½¬æ¢ï¼ˆV5 Turboç‰ˆæœ¬ï¼‰
    
    ä¸»è¦ä¼˜åŒ–:
    - æ¿€è¿›å¹¶å‘é…ç½®ï¼ˆ25+å¹¶å‘ï¼‰
    - æ™ºèƒ½è´Ÿè½½å‡è¡¡
    - å›¾ç‰‡ç¼“å­˜ä¼˜åŒ–
    - è‡ªé€‚åº”è¶…æ—¶
    - å›¾ç‰‡å‹ç¼©
    """
    processor = PDFVLMProcessorV5Turbo(
        pdf_workers=pdf_workers,
        vlm_workers=vlm_workers,
        batch_size=batch_size,
        initial_concurrency=initial_concurrency,
        max_concurrency=max_concurrency,
        dpi=dpi,
        model=model,
        enable_image_compression=enable_compression
    )
    
    return processor.process_pdf_to_markdown_turbo(
        pdf_path=pdf_path,
        output_md_path=output_md_path,
        cleanup_images=cleanup_images
    )

# å¤šè¿›ç¨‹å·¥ä½œå‡½æ•°éœ€è¦åœ¨æ¨¡å—çº§åˆ«å®šä¹‰
def _convert_page_range_worker(args: Tuple[str, int, int, str, str, int]) -> List[Tuple[int, str]]:
    """å¤šè¿›ç¨‹PDFè½¬å›¾ç‰‡å·¥ä½œå‡½æ•°"""
    pdf_path, start_page, end_page, output_dir, file_prefix, dpi = args
    
    try:
        start_time = time.time()
        
        images = convert_from_path(
            pdf_path,
            dpi=dpi,
            first_page=start_page,
            last_page=end_page,
            fmt='PNG'
        )
        
        results = []
        for i, image in enumerate(images):
            page_num = start_page + i
            output_path = os.path.join(output_dir, f"{file_prefix}_page_{page_num:04d}.png")
            
            # ä¼˜åŒ–ä¿å­˜å‚æ•°
            image.save(output_path, 'PNG', optimize=True, compress_level=3)
            results.append((page_num, output_path))
            
        end_time = time.time()
        logger.debug(f"è½¬æ¢é¡µé¢ {start_page}-{end_page} å®Œæˆï¼Œè€—æ—¶ {end_time - start_time:.2f}s")
        
        return results
        
    except Exception as e:
        logger.error(f"è½¬æ¢é¡µé¢ {start_page}-{end_page} å¤±è´¥: {str(e)}")
        return []

if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    if not os.environ.get("DOUBAO_API_KEY"):
        print("é”™è¯¯ï¼šè¯·è®¾ç½® DOUBAO_API_KEY ç¯å¢ƒå˜é‡")
        sys.exit(1)
    
    # æµ‹è¯•æ–‡ä»¶
    pdf_file = r"D:\AgentBuilding\FinAgent\files\arxiv_papers\2506.19676v3.pdf"
    output_md_file = r"D:\AgentBuilding\FinAgent\files\arxiv_papers\2506.19676v3_turbo.md"
    
    try:
        logger.info("ğŸš€ å¼€å§‹ä½¿ç”¨æé€Ÿç‰ˆæœ¬å¤„ç†PDF...")
        
        markdown_content = convert_pdf_to_markdown_v5_turbo(
            pdf_path=pdf_file,
            output_md_path=output_md_file,
            dpi=150,                    # é€‚ä¸­çš„DPI
            batch_size=15,              # æ¿€è¿›çš„æ‰¹æ¬¡å¤§å°
            initial_concurrency=25,     # æ¿€è¿›çš„åˆå§‹å¹¶å‘
            max_concurrency=40,         # æ›´é«˜çš„æœ€å¤§å¹¶å‘
            pdf_workers=6,              # æ›´å¤šPDFå¤„ç†è¿›ç¨‹
            vlm_workers=8,              # æ›´å¤šVLMå¤„ç†è¿›ç¨‹
            enable_compression=True,    # å¯ç”¨å›¾ç‰‡å‹ç¼©
            cleanup_images=True
        )
        
        logger.info("ğŸ‰ æé€Ÿç‰ˆæœ¬å¤„ç†å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {str(e)}")
        sys.exit(1) 